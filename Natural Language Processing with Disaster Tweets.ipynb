{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc0a83-3aa2-4d6b-9f49-5a42b6261427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from wordcloud import STOPWORDS\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9106936-0c93-442e-9380-bfc5bf601da4",
   "metadata": {},
   "source": [
    "### Brief description of the problem and data\n",
    "\n",
    "Briefly describe the challenge problem and NLP. Describe the size, dimension, structure, etc., of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584460f5-a3ec-47ac-9eab-3e4da58bf3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train_copy = train.copy()\n",
    "test_copy = test.copy()\n",
    "print('Sample of Test:\\n')\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf3ed9-a17d-41b6-b08b-5eaaf6d2bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample of Train:\\n')\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f9a14-9c01-43a8-b59b-9b0a4b58c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\n",
    "print('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5545cd4-0a62-466f-8b17-f7adcc16f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c73653-3faa-404c-9bcf-09c3f0ad7498",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc97940-d086-4f1d-a122-def25f6686a3",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA) — Inspect, Visualize and Clean the Data\n",
    "\n",
    "Show a few visualizations like histograms. Describe any data cleaning procedures. Based on your EDA, what is your plan of analysis? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6e2b8-2f3b-4024-9809-eb08e8f2427d",
   "metadata": {},
   "source": [
    "#### Class Imbalance: Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b996f74-3162-4865-8f8a-e46a4649a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "cnt_srs = train['target'].value_counts()\n",
    "cnt_srs.plot(kind='bar', color='skyblue', ax=axs[0])\n",
    "axs[0].set_title('Target Count')\n",
    "axs[0].set_xlabel('Target')\n",
    "axs[0].set_ylabel('Count')\n",
    "axs[0].tick_params(axis='x', rotation=180)\n",
    "\n",
    "for i, count in enumerate(cnt_srs):\n",
    "    axs[0].text(i, count + 0.1, str(count), ha='center', va='bottom')\n",
    "\n",
    "axs[1].pie(cnt_srs, labels=cnt_srs.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.tab20.colors)\n",
    "axs[1].set_title('Target Distribution')\n",
    "axs[1].axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb33b74-5669-4414-8db2-4d83e13231f0",
   "metadata": {},
   "source": [
    "The target column in the training dataset indicates that there are 4,342 records classified as 0 (non-disaster tweets) and 3,271 records classified as 1 (disaster tweets). This distribution translates to 43% for disaster tweets and 57% for non-disaster tweets. The data exhibits a slight imbalance, with a higher proportion leaning towards non-disaster tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f0807-4aaa-4d15-bb89-0076f5fa2480",
   "metadata": {},
   "source": [
    "#### Non-Disaster and Disaster Tweet by Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1cdc51-8a80-4df8-a7e9-57cb51fa9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster = train[train[\"target\"] == 1]\n",
    "non_disaster = train[train[\"target\"] == 0]\n",
    "\n",
    "cnt_disaster = disaster['location'].value_counts().head(20)\n",
    "cnt_non_disaster = non_disaster['location'].value_counts().head(20)\n",
    "\n",
    "x = np.arange(len(cnt_non_disaster))\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "bar_width = 0.35  \n",
    "plt.bar(x - bar_width/2, cnt_non_disaster.values, bar_width, color='skyblue', label=\"Non-Disaster\")\n",
    "plt.bar(x + bar_width/2, cnt_disaster.values, bar_width, color='salmon', label=\"Disaster\")\n",
    "\n",
    "for i, v in enumerate(cnt_non_disaster.values):\n",
    "    plt.text(i - bar_width/2, v, str(v), ha='center', va='bottom')\n",
    "for i, v in enumerate(cnt_disaster.values):\n",
    "    plt.text(i + bar_width/2, v, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.title('Number of Tweets by Location')\n",
    "plt.xlabel('Location')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.xticks(ticks=x, labels=cnt_non_disaster.index, rotation=90)  # Set the tick positions and labels\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e7cf7d-826c-4c92-965b-976b6dc28ef1",
   "metadata": {},
   "source": [
    "The location with the highest number of tweets, both related to disasters and non-disasters, is New York state, with variations such as \"New York,\" \"NYC,\" and \"New York, NY\" comprising a total of 73 non-disaster tweets and 83 disaster tweets. California ranks as the state with the second-highest tweet count, with variations like \"Los Angeles, CA,\" \"California,\" and \"San Francisco\" accumulating 36 non-disaster tweets and 38 disaster tweets. \n",
    "\n",
    "The United States ranks as the country with the highest tweet count, with variations like \"USA\" and \"United States\" totaling 60 non-disaster tweets and 47 disaster tweets not including individual cities or states. On the international scale, the United Kingdom secured the second spot, with variations like \"UK\" and \"United Kingdom\" not including individual cities or provinces, amassing 21 non-disaster tweets and 22 disaster tweets.\n",
    "\n",
    "These numbers suggest that urban hubs and densely populated regions tend to generate more tweet activity, possibly due to a higher concentration of users and a greater likelihood of both disasters and non-disaster events occurring. Additionally, it highlights the global reach and significance of platforms like Twitter in capturing real-time information and discussions around various events and incidents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7be02b-da41-4b12-a28d-9ee8c2c8dcd6",
   "metadata": {},
   "source": [
    "#### Word Length Density for Non-Disaster vs Disaster Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44700a99-ac7d-40dd-aa58-552a03bd5caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"use_inf_as_na option is deprecated\", category=FutureWarning)\n",
    "train['length'] = train['text'].apply(len) \n",
    "plt.rcParams['figure.figsize'] = (10.0, 6.0)\n",
    "sns.kdeplot(train[train['target'] == 0]['length'], fill=True, label='Non-Disaster')\n",
    "sns.kdeplot(train[train['target'] == 1]['length'], fill=True, label='Disaster')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Density Plot of Text Length for Different Targets')\n",
    "plt.xlim(0, 150)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb23cde-2130-4eb4-80ab-91091acb6b4f",
   "metadata": {},
   "source": [
    "The density of non-disaster tweets reaches its highest point, around 0.015, when the word length is approximately 138 words. Initially, there's a slight peak at a density of around 0.005, occurring around 40 words, followed by a more significant peak at 0.015. After this peak, the density sharply declines.\n",
    "\n",
    "In contrast, disaster tweets exhibit a different pattern. Their density surpasses 0.020 at around 138 words, indicating a higher concentration of information compared to non-disaster tweets. The density plot for disaster tweets shows a steady increase, with two notable peaks. The first peak, slightly smaller, occurs at around 90 words with a density of about 0.009. The second peak is more pronounced, reaching above 0.020. Following this peak, the density sharply declines.\n",
    "\n",
    "These numbers suggest that disaster tweets tend to be more densely packed with information compared to non-disaster tweets, especially evident in the second peak. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c42c2-b8b6-43a5-a0f5-2b892bbc2dcb",
   "metadata": {},
   "source": [
    "#### Word, UniqueWord, StopWord Text Distribution in Non-Disaster and Disaster Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6400580-0dcb-4de9-a08e-8919f5695b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return str(text).lower()\n",
    "\n",
    "def calculate_text_features(df):\n",
    "    df['Word_Count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "    df['Unique_Word_Count'] = df['text'].apply(lambda x: len(set(str(x).split())))\n",
    "    df['Stop_Word_Count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "    df['Url_Count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "    df['Mean_Word_Length'] = df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    df['Char_Count'] = df['text'].apply(lambda x: len(str(x)))\n",
    "    df['Punctuation_Count'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    df['Hashtag_Count'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "    df['Mention_Count'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "def plot_feature_distributions(train, feature, ax):\n",
    "    sns.histplot(train[train['target'] == 0][feature], label='Non-Disaster', ax=ax, color='green', kde=True)\n",
    "    sns.histplot(train[train['target'] == 1][feature], label='Disaster', ax=ax, color='red', kde=True)\n",
    "    ax.set_xlabel(f'{feature}',fontsize=12)\n",
    "    ax.set_ylabel('Text Density',fontsize=12)\n",
    "    ax.tick_params(axis='x', labelsize=12)\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    ax.legend()\n",
    "    ax.set_title(f'{feature} Text Distribution', fontsize=13)\n",
    "    for container in ax.containers:\n",
    "        for bar in container:\n",
    "            height = bar.get_height()\n",
    "            if height != 0:\n",
    "                ax.text(\n",
    "                    bar.get_x() + bar.get_width() / 2,  \n",
    "                    height,                             \n",
    "                    f'{height:.0f}',                    \n",
    "                    ha='center',                       \n",
    "                    va='bottom',                      \n",
    "                    color=bar.get_facecolor(),         \n",
    "                    fontsize=10,\n",
    "                    fontweight='bold'\n",
    "                )\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, figsize=(10, 15), dpi=100)\n",
    "\n",
    "train['text'] = train['text'].apply(preprocess_text)\n",
    "calculate_text_features(train)\n",
    "features_to_plot = ['Word_Count', 'Unique_Word_Count', 'Stop_Word_Count']\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    plot_feature_distributions(train, feature, axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92459e77-b74e-4216-8abd-0a82719d44be",
   "metadata": {},
   "source": [
    "The Word Text Distribution Graph illustrates the distribution of text density concerning the word count in both Non-Disaster and Disaster tweets. Notably, both types of tweets exhibit similar patterns, with their peak text densities occurring around a word count of 16. Before reaching this peak, both graph lines gradually ascend, hitting their initial peak around word count 11, with Disaster tweets slightly lower at 221 text density compared to Non-Disaster tweets at 276. However, Disaster tweets surpass Non-Disaster tweets in text density only at 17 word count, with 277 for Disaster tweets and 217 for Non-Disaster tweets.\n",
    "\n",
    "In the Unique Word Text Distribution Graph, there's a distinct gap in unique word count for Disaster tweets around 14 words. Initially, both Disaster and Non-Disaster tweet lines rise steadily, reaching their first peak at around 13 unique words and 259 text density for Disaster tweets. After the gap, Disaster tweet lines slightly decrease before rising again to their second peak at around 18 unique words and 257 text density. Conversely, Non-Disaster tweet lines ascend steadily until they peak at approximately 15 unique words and 269 text density, followed by a gradual decrease. Disaster tweets outpace Non-Disaster tweets in text density only between 17 to 18 unique word count.\n",
    "\n",
    "The Stop Word Text Distribution Graph indicates the distribution of text density concerning the count of stop words in both types of tweets. Both Disaster and Non-Disaster graph lines follow a similar pattern of gradual increase until they reach their respective peaks, followed by a decline. Non-Disaster tweets peak at a text density of 571 and approximately 2.5 stop words, while Disaster tweets peak at 511 text density and around 2.3 stop words. Disaster tweets exceed Non-Disaster tweets in text density only around 1.0 stop word count, with 462 for Disaster tweets and 430 for Non-Disaster tweets.\n",
    "\n",
    "These distributions reveal intriguing patterns in how Disaster and Non-Disaster tweets are structured concerning text density and various linguistic aspects. Despite some fluctuations, Disaster tweets generally exhibit slightly lower text densities compared to Non-Disaster tweets across different linguistic measures, except for specific word or stop word counts where Disaster tweets temporarily surpass Non-Disaster tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea244a4-65b2-4292-8ab4-b6f111f1e176",
   "metadata": {},
   "source": [
    "#### URL, MeanWordLength, Characters Text Distribution in Non-Disaster and Disaster Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c95935-eda0-4676-93f0-6a1825492344",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, figsize=(10, 15), dpi=100)\n",
    "\n",
    "train['text'] = train['text'].apply(preprocess_text)\n",
    "calculate_text_features(train)\n",
    "features_to_plot = ['Url_Count', 'Mean_Word_Length', 'Char_Count']\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    plot_feature_distributions(train, feature, axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5bcdd4-4b83-4dc3-9849-5315594e60d4",
   "metadata": {},
   "source": [
    "In examining the URL Text Distribution graph, distinct patterns emerge at various URL count intervals. Initially, at around 0.2 URL count, the text density for Disaster tweets is notably lower than that of Non-Disaster tweets, with 1099 and 2543 respectively. However, a shift occurs at the next data point of 1.0 URL count, where Disaster tweets surpass Non-Disaster tweets in text density, registering 1835 and 1413 respectively. The trend continues with a third data point at 2.0 URL count, where Non-Disaster tweets exhibit a higher text density than Disaster tweets, recording 370 and 328 respectively. Beyond this threshold, text density becomes sparser for both Disaster and Non-Disaster tweets.\n",
    "\n",
    "Moving to the Mean Word Length text distribution graph, a notable contrast emerges in the pace at which Non-Disaster and Disaster tweets reach their peak text densities. Non-Disaster tweets accelerate swiftly to their apex, reaching 351 text density at a mean word length of 3.0, followed by a gradual decline. Conversely, Disaster tweets experience a more gradual ascent, culminating at approximately 254 text density around a mean word length of 6.0.\n",
    "\n",
    "Examining the Char Count text distribution graph reveals a consistent trend of steady increase followed by a decline in text density for both Non-Disaster and Disaster tweets. Non-Disaster tweets peak at 723 text density and 140 character count, while Disaster tweets reach their zenith at 734 text density and 135 character count. Notably, there's a point of divergence where Disaster tweets surpass Non-Disaster tweets at 145 text character count, with 307 Disaster tweets compared to 84 Non-Disaster tweets.\n",
    "\n",
    "Insights gleaned from all three graphs suggest nuanced differences in text characteristics between Disaster and Non-Disaster tweets. Despite some fluctuations, Disaster tweets tend to exhibit higher text density and longer mean word lengths compared to Non-Disaster tweets, particularly evident at specific thresholds such as URL count intervals and character counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b912bd-2997-4b40-a7b4-a55e3d268125",
   "metadata": {},
   "source": [
    "#### Punctuation, Hashtag, Mentions Text Distribution in Non-Disaster and Disaster Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64bea9-683c-4485-b331-2d1ff0715a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, figsize=(10, 15), dpi=100)\n",
    "\n",
    "train['text'] = train['text'].apply(preprocess_text)\n",
    "calculate_text_features(train)\n",
    "features_to_plot = ['Punctuation_Count', 'Hashtag_Count', 'Mention_Count']\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    plot_feature_distributions(train, feature, axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96a38e-3d2e-4e97-be2c-e85f0f54c2c0",
   "metadata": {},
   "source": [
    "In the punctuation text distribution graph, both non-Disaster and Disaster tweets exhibit three distinct peaks. Non-Disaster tweets initially ascend steadily to the first peak at 400 text density and 2 punctuation count, then gradually decline to the second peak at 397 text density and 5 punctuation count, before tapering off to the third peak at 240 text density and 10 punctuation count. Conversely, Disaster tweets follow a different trajectory, ascending steadily to their first peak at 163 text density and 3 punctuation count, then sharply escalating to the second peak at 379 text density and 6 punctuation count, before slowly declining to their third peak at 200 text density and 11 punctuation count.\n",
    "\n",
    "In the Hashtag count distribution graph, non-Disaster tweets initially dominate, with a text density of 3456 compared to 2396 for Disaster tweets, at 0 to 1 hashtag count. However, both lines quickly level off, showing similar text density of around 482 as the hashtag count increases. This trend continues as the hashtag count rises.\n",
    "\n",
    "For the mention count distribution graph, non-Disaster tweets start with a higher text density (2979) compared to Disaster tweets (2595) at 0 to 0.5 mention count. The density then decreases steadily. Interestingly, at 3 mention count, both Disaster and non-Disaster tweets converge to similar text density, around 416, as the mention count decreases towards 0.\n",
    "\n",
    "These distributions reveal nuanced patterns in tweet characteristics. The punctuation graph suggests that Disaster tweets tend to have more intense bursts of punctuation compared to non-Disaster tweets. Additionally, the Hashtag count graph illustrates a convergence of text density as hashtag count increases, indicating a similar engagement with hashtags across both tweet types. Lastly, the mention count graph showcases a shift from initially higher text density in non-Disaster tweets to eventual parity with Disaster tweets as mention count rises, hinting at a convergence in communication style as mention count increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e9de6-6d1e-40c6-bfb7-c02648346926",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return text\n",
    "\n",
    "def generate_ngrams(text, n=1):\n",
    "    tokens = text.split()\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "def count_ngrams(texts, n):\n",
    "    ngram_counts = defaultdict(int)\n",
    "    for text in texts:\n",
    "        text = preprocess_text(text)\n",
    "        for ngram in generate_ngrams(text, n):\n",
    "            ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "def plot_top_ngrams(ngram_counts, title, color, N, ax=None):\n",
    "    counter = Counter(ngram_counts)\n",
    "    df_ngrams = pd.DataFrame(counter.most_common(N), columns=['Ngram', 'Frequency'])\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Frequency', y='Ngram', data=df_ngrams, palette=color)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Frequency')\n",
    "        plt.ylabel('Ngram')\n",
    "    else:\n",
    "        sns.barplot(x='Frequency', y='Ngram', data=df_ngrams, palette=color, ax=ax)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Frequency')\n",
    "        ax.set_ylabel('Ngram')\n",
    "\n",
    "    for i, (_, freq) in enumerate(df_ngrams.iterrows()):\n",
    "        ax.text(freq['Frequency'], i, freq['Frequency'], color='black', ha=\"left\", va=\"center\")\n",
    "\n",
    "\n",
    "N = 20  \n",
    "disaster_texts = train[train['target']==1]['text']\n",
    "non_disaster_texts = train[train['target']==0]['text']\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(20, 25))\n",
    "\n",
    "disaster_unigrams = count_ngrams(disaster_texts, 1)\n",
    "non_disaster_unigrams = count_ngrams(non_disaster_texts, 1)\n",
    "plot_top_ngrams(disaster_unigrams, f'Top {N} most common unigrams for Disaster Tweets', 'pastel', N, ax=axs[0, 0])\n",
    "plot_top_ngrams(non_disaster_unigrams, f'Top {N} most common unigrams for Non-Disaster Tweets', 'muted', N, ax=axs[0, 1])\n",
    "\n",
    "disaster_bigrams = count_ngrams(disaster_texts, 2)\n",
    "non_disaster_bigrams = count_ngrams(non_disaster_texts, 2)\n",
    "plot_top_ngrams(disaster_bigrams, f'Top {N} most common bigrams for Disaster Tweets', 'pastel', N, ax=axs[1, 0])\n",
    "plot_top_ngrams(non_disaster_bigrams, f'Top {N} most common bigrams for Non-Disaster Tweets', 'muted', N, ax=axs[1, 1])\n",
    "\n",
    "disaster_trigrams = count_ngrams(disaster_texts, 3)\n",
    "non_disaster_trigrams = count_ngrams(non_disaster_texts, 3)\n",
    "plot_top_ngrams(disaster_trigrams, f'Top {N} most common trigrams for Disaster Tweets', 'pastel', N, ax=axs[2, 0])\n",
    "plot_top_ngrams(non_disaster_trigrams, f'Top {N} most common trigrams for Non-Disaster Tweets', 'muted', N, ax=axs[2, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbea78-e019-485e-8ea6-73edb9bcc2ca",
   "metadata": {},
   "source": [
    "In the unigram graphs, the word 'the' reigns supreme, boasting the highest frequency in both Disaster and Non-Disaster Tweets, with 1357 and 1905 occurrences, respectively. Following closely behind, 'in' secures the second spot in Disaster Tweets with 1159 occurrences, while 'a' takes the runner-up position in Non-Disaster Tweets with 1252 occurrences. 'A' maintains its significance as the third most common word in Disaster Tweets, appearing 924 times, whereas 'to' claims this spot in Non-Disaster Tweets with 1188 occurrences. 'Of' clinches the fourth position in Disaster Tweets with 922 appearances, while 'i' takes its place in Non-Disaster Tweets with 1076 occurrences. Finally, 'to' emerges as the fifth most frequent word in Disaster Tweets (757 occurrences), while 'and' secures this rank in Non-Disaster Tweets (917 occurrences).\n",
    "\n",
    "In the bigram graphs, 'in the' emerges as the dominant pair, boasting the highest frequency in both Disaster and Non-Disaster Tweets, with 144 and 163 occurrences, respectively. Following closely behind, 'of the' secures the second position in both Disaster and Non-Disaster Tweets, with 118 and 137 occurrences, respectively. In Disaster Tweets, 'suicide bomber' claims the third spot with 59 occurrences, whereas 'to the' takes this place in Non-Disaster Tweets with 89 occurrences. 'On the' follows suit as the fourth most common bigram in both Disaster and Non-Disaster Tweets, with 51 and 78 occurrences, respectively. Lastly, in Disaster Tweets, 'in a' emerges as the fifth most frequent bigram, appearing 50 times, while 'to be' secures this position in Non-Disaster Tweets with 73 occurrences.\n",
    "\n",
    "Moving onto trigram graphs, phrases like 'northern california wildfire,' 'more homes razed,' and 'homes razed by' dominate the Disaster Tweets with the highest word frequency of 29. Conversely, in Non-Disaster Tweets, phrases like 'i liked a,' 'liked a youtube,' and 'a youtube video' claim the top spot with a frequency of 35. Additionally, phrases such as 'the latest more,' 'latest more homes,' 'razed by northern,' 'pkk suicide bomber,' 'suicide bomber who,' 'bomber who detonated,' and 'detonated bomb in' emerge as the second highest in Disaster Tweets with a frequency of 28. Meanwhile, 'reddit will now' and 'will now quarantine' hold this position in Non-Disaster Tweets with a frequency of 21.\n",
    "\n",
    "These graphs showcase the nuanced differences in language usage between Disaster and Non-Disaster contexts. Disaster-related content tends to feature terms like 'wildfire,' 'homes razed,' and 'suicide bomber,' reflecting the urgency and gravity of such situations. Conversely, Non-Disaster Tweets are marked by more commonplace language, with phrases like 'liked a youtube video' and 'reddit will now quarantine' dominating the discourse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd660d-c070-4473-a2af-f0976e9f5917",
   "metadata": {},
   "source": [
    "#### Data Cleaning and Standardized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1fe6ca-6e8f-4854-a780-7e56c29f42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map \n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def cleaned_text(text):\n",
    "    text = text.lower()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(table)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) \n",
    "    text = re.sub(r'<.*?>', '', text) \n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "# Sample usage:\n",
    "df = pd.concat([train_copy, test_copy])\n",
    "df['text'] = df['text'].apply(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1031e1e-effb-4dca-a045-e7522e6c3db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "df['text']=df['text'].apply(lambda x : correct_spellings(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd65abe1-d236-4d73-9602-8fea47a0e8a2",
   "metadata": {},
   "source": [
    "- Lowercasing: text = text.lower() converts all characters in the text to lowercase.\n",
    "- Removing Punctuation: re.sub(r'[^\\w\\s]', '', text) removes any characters that are not alphanumeric or whitespace, including punctuation marks like periods, commas, and exclamation marks.\n",
    "- Removing URLs: re.sub(r'https?://\\S+|www\\.\\S+', '', text) removes URLs, matching both HTTP and HTTPS URLs as well as URLs without a protocol.\n",
    "- Removing HTML tags: re.sub(r'<.*?>', '', text) removes HTML tags.\n",
    "- Removing Emojis: emoji_pattern.sub(r'', text) removes emojis based on their Unicode code points.\n",
    "- Tokenization: word_tokenize(text) tokenizes the text into individual words, spliting the text into words based on whitespace and punctuation.\n",
    "- Removing Stopwords: [token for token in tokens if token not in stop_words] removes stopwords from the tokens, like \"the\", \"is\", \"and\", etc., that often don't carry much meaning.\n",
    "- Stemming: [stemmer.stem(token) for token in tokens] stems each token using a stemming algorithm, reduces words to their root or base form, which helps in reducing the vocabulary size and capturing the essence of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d13d6-6b34-4ec4-a94c-a838ae74ce5f",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "Describe your model architecture and reasoning for why you believe that specific architecture would be suitable for this problem. \n",
    "\n",
    "Since we did not learn NLP-specific techniques such as word embeddings in the lectures, we recommend looking at Kaggle tutorials, discussion boards, and code examples posted for this challenge.  You can use any resources needed, but make sure you “demonstrate” you understood by including explanations in your own words. Also importantly, please have a reference list at the end of the report.  \n",
    "\n",
    "There are many methods to process texts to matrix form (word embedding), including TF-IDF, GloVe, Word2Vec, etc. Pick a strategy and process the raw texts to word embedding. Briefly explain the method(s) and how they work in your own words.\n",
    "\n",
    "Build and train your sequential neural network model (You may use any RNN family neural network, including advanced architectures LSTM, GRU, bidirectional RNN, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bebefa-5a5e-4c26-9a4c-f5bbb3db8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "# Load GloVe embeddings\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['text']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "    \n",
    "corpus=create_corpus(df)\n",
    "embedding_dict = {word: word_vectors[word] for word in word_vectors.key_to_index if word in corpus}\n",
    "\n",
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))\n",
    "\n",
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    emb_vec = embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i] = emb_vec\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimzer=Adam(learning_rate=1e-5)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.2, random_state=42)\n",
    "print('Shape of train', X_train.shape)\n",
    "print(\"Shape of Validation \", X_test.shape)\n",
    "history = model.fit(X_train, y_train, batch_size=4, epochs=15, validation_data=(X_test, y_test), verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb142e7d-1411-451a-8cf8-b6e32fd6b446",
   "metadata": {},
   "source": [
    "### Results and Analysis\n",
    "\n",
    "Run hyperparameter tuning, try different architectures for comparison, apply techniques to improve training or performance, and discuss what helped.\n",
    "\n",
    "Includes results with tables and figures. There is an analysis of why or why not something worked well, troubleshooting, and a hyperparameter optimization procedure summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d2598-a977-47bb-910f-fb001f5adb6b",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Discuss and interpret results as well as learnings and takeaways. What did and did not help improve the performance of your models? What improvements could you try in the future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbc839-8647-410f-b440-cf020d7767a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
